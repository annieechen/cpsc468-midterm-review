\subsection*{Chapter 1: The Computational Model and Why It Doesn't Matter}
\begin{note}[Turing Machine]
  A $k$-tape Turing Machine $M$ is described by a tuple $(\Gamma, Q, \delta)$.
  Assume $k \geq 2$, with $1$ read-only input tape and $k - 1$ work tapes.
  The last work tape is assumed to be the output tape.
  \begin{itemize}
    \item
      $\Gamma$ the finite alphabet of symbols that $M$ may have on its tapes.
      Assume that $\Gamma$ contains at least $\{0,1,\square,\triangleright\}$.

    \item
      $Q$ a finite set of possible states $M$'s state register may be in.
      Assume that $Q$ contains a $q_{start}$ and $q_{halt}$.

    \item
      $\delta : Q \times \Gamma^k \to Q
      \times \Gamma^{k - 1} \times \{L, S, R\}^k$ a transition function for
      $M$ that takes in the current state and each head's read, and outputs
      the next state, with $k - 1$ writes on all the work tapes, and movements
      for all $k$ tapes.
  \end{itemize}
\end{note}

\begin{note}[Computing a Function]
  Let $f : \{0, 1\}^\ast \to \{0, 1\}^\ast$ and
  $T : \mathbb{N} \to \mathbb{N}$, with $M$ a TM.
  We say that $M$ computes $f$ if for every $x \in \{0, 1\}^\ast$, if
  $M$ is initialized to the start configuration on input $x$, then it halts
  with $f(x)$ on the output tape.
  We say $M$ computes $f$ in $T(n)$-time if its computation on every
  $x$ requires at most $T(|x|)$ steps.
\end{note}

\begin{note}[Time Constructible]
  A function $T: \mathbb{N} \to \mathbb{N}$ is time constructible if
  $T(n) \geq n$ and there is a TM $M$ that computes the function
  $x \mapsto \llcorner T(|x|) \lrcorner$ in time $T(n)$.
  $T(n) \geq n$ is to allow the algorithm to read its input.
  Some time constructible functions are $n$, $n \log n$, $n^2$ and $2^n$.
\end{note}

\begin{note}
  For every $f : \{0, 1\}^\ast \to \{0, 1\}$ and time constructible
  $T : \mathbb{N} \to \mathbb{N}$, if $f$ is computable in time $T(n)$ by
  some TM $M$ using alphabet $\Gamma$, then it is able to compute the same
  function using $\{0, 1, \square, \triangleright\}$ in
  $\left(c \log_2 | \Gamma| \right) \cdot T(n)$.
  This is because we may express each symbol of $\Gamma$ using
  $\log |\Gamma|$ binary bits, with some constant $c$ overhead.
\end{note}

\begin{note}
  A $k$-tape TM can have its $k - 1$ work tapes simulated by a single tape by
  interleaving the $k$ tapes together.
\end{note}

\begin{note}[Oblivious Turing Machine]
  An oblivious TM's head movement depends on the length of the input, not
  the contents of the input.
  Every TM can be simulated by an oblivious TM.
\end{note}

\begin{note}[Turing Machine Representation]
  Every binary string $x \in \{0, 1\}^\ast$ represents some TM, and every TM
  is represented by infinite such strings (think: comments in a language).
  The machine represented by $x$ is denoted $M_x$.
\end{note}

\begin{note}[Universal Turing Machine]
  There exists a TM $\mathcal{U}$ such that for every
  $x, \alpha \in \{0, 1\}^\ast$, $\mathcal{U}(x, a) = M_\alpha (x)$, where
  $M_\alpha$ denotes the TM represented by $\alpha$.
  Moreover, if $M_\alpha$ halts on input $x$ within $T$ steps, then
  $\mathcal{U}_\alpha (x)$ halts within $C T \log T$ steps, where $C$ is
  a number independent of $|x|$, and depends only on $M_\alpha$'s alphabet
  size, number of tapes, and number of states.
  The cost of simulating any machine $M_\alpha$ has a
  logarithmic overhead, due to the alphabet size difference between
  $M_\alpha$ and $\mathcal{U}$.
  As $\mathcal{U}$ has a single tape, we do the trick over interleaving
  $M_\alpha$'s work tapes together.
\end{note}

\begin{note}[Uncomputable Function]
  Define $U$ as follows: for every $\alpha \in \{0, 1\}^\ast$, if the machine
  defined by $\alpha$ accepts itself, such that $M_\alpha (\alpha) = 1$, then
  $U(\alpha) = 0$.
  In other words $U(\alpha) = 1 - M_\alpha (\alpha)$.
  There is no such TM that can compute $U$, because $U$ will always negate it.
\end{note}

\begin{note}[Halting Problem]
  A TM $H$ such that $H(\alpha, x) = 1$ if $M_\alpha (x)$ halts, and yields
  $0$ otherwise, does not exist.
  We can construct a wrapper TM $W$ that invokes $H$ on itself, and performs
  the opposite.
  Diagonalization motherfuckers!
\end{note}

\begin{note}[$\DTIME$]
  Let $T : \mathbb{N} \to \mathbb{N}$ be some function.
  A language $L$ is in $\DTIME(T(n))$ iff there is a deterministic
  TM that runs in time
  $c \cdot T(n)$ for some constant $c > 0$ and decides $L$.
  This class contains \textbf{decision} problems.
\end{note}

\begin{note}[The Class $\PTIME$]
  $\PTIME = \bigcup_{c \geq 1} \DTIME(n^c)$
\end{note}

\begin{note}[Church-Turing Thesis]
  Every physically realizable computation device can be simulated by a TM.
\end{note}

\begin{note}[Bounds]
  The asymptoptic operators $\{o, O, \Theta, \Omega, \omega\}$ can be thought
  of as $\{ <, \leq, =, \geq, > \}$.
\end{note}
